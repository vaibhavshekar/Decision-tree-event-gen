{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 153\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m    152\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m10k_single_agent.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 153\u001b[0m predictions, X_test, y_test, vocabularies \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_event_log\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;66;03m# Print the results\u001b[39;00m\n\u001b[0;32m    156\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted sequence:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 129\u001b[0m, in \u001b[0;36mprocess_event_log\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_event_log\u001b[39m(file_path):\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;66;03m# Load and preprocess data\u001b[39;00m\n\u001b[0;32m    128\u001b[0m     df \u001b[38;5;241m=\u001b[39m load_data(file_path)\n\u001b[1;32m--> 129\u001b[0m     processed_df, grid, vocabularies \u001b[38;5;241m=\u001b[39m preprocess_data(df)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# Prepare target variables\u001b[39;00m\n\u001b[0;32m    132\u001b[0m     y \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mget_dummies(processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_type\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m    134\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_id\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mget_dummies(processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent_id\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues,\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: pd\u001b[38;5;241m.\u001b[39mget_dummies(processed_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m    136\u001b[0m     }\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data(file_path):\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    le = LabelEncoder()\n",
    "    df['event_type_encoded'] = le.fit_transform(df['event_type'])\n",
    "    df['agent_id_encoded'] = le.fit_transform(df['agent_id'])\n",
    "    df['context_encoded'] = le.fit_transform(df['context'])\n",
    "    \n",
    "    vocabularies = {\n",
    "        'event_type': df['event_type'].unique().tolist(),\n",
    "        'agent_id': df['agent_id'].unique().tolist(),\n",
    "        'context': df['context'].unique().tolist()\n",
    "    }\n",
    "    \n",
    "    return df, vocabularies\n",
    "\n",
    "def create_cnn_model(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    x = Conv2D(64, (2, 2), activation='relu', padding='same')(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(64, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Conv2D(128, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Conv2D(128, (2, 2), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = Dropout(0.25)(x)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    \n",
    "    return Model(inputs=inputs, outputs=x)\n",
    "\n",
    "class HierarchicalModel:\n",
    "    def __init__(self, input_shape, vocabularies):\n",
    "        self.cnn = create_cnn_model(input_shape)\n",
    "        self.vocabularies = vocabularies\n",
    "        \n",
    "        # Event type prediction\n",
    "        event_input = Input(shape=(256,))\n",
    "        event_output = Dense(len(vocabularies['event_type']), activation='softmax', name='event_type')(event_input)\n",
    "        self.event_model = Model(inputs=event_input, outputs=event_output)\n",
    "        \n",
    "        # Agent ID prediction\n",
    "        agent_input = Input(shape=(256 + len(vocabularies['event_type']),))\n",
    "        agent_output = Dense(len(vocabularies['agent_id']), activation='softmax', name='agent_id')(agent_input)\n",
    "        self.agent_model = Model(inputs=agent_input, outputs=agent_output)\n",
    "        \n",
    "        # Context prediction\n",
    "        context_input = Input(shape=(256 + len(vocabularies['event_type']) + len(vocabularies['agent_id']),))\n",
    "        context_output = Dense(len(vocabularies['context']), activation='softmax', name='context')(context_input)\n",
    "        self.context_model = Model(inputs=context_input, outputs=context_output)\n",
    "        \n",
    "        self.compile_models()\n",
    "    \n",
    "    def compile_models(self):\n",
    "        optimizer = Adam(learning_rate=0.001)\n",
    "        self.event_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.agent_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.context_model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    def fit(self, X, y, epochs=50, batch_size=64):\n",
    "        cnn_features = self.cnn.predict(X)\n",
    "        \n",
    "        # Train event type model\n",
    "        self.event_model.fit(cnn_features, y['event_type'], epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "        \n",
    "        # Train agent ID model\n",
    "        event_pred = self.event_model.predict(cnn_features)\n",
    "        agent_input = np.concatenate([cnn_features, event_pred], axis=1)\n",
    "        self.agent_model.fit(agent_input, y['agent_id'], epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "        \n",
    "        # Train context model\n",
    "        agent_pred = self.agent_model.predict(agent_input)\n",
    "        context_input = np.concatenate([cnn_features, event_pred, agent_pred], axis=1)\n",
    "        self.context_model.fit(context_input, y['context'], epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "    \n",
    "    def predict_sequence(self, initial_input, sequence_length=5):\n",
    "        predictions = []\n",
    "        current_input = initial_input\n",
    "        \n",
    "        for _ in range(sequence_length):\n",
    "            cnn_features = self.cnn.predict(current_input)\n",
    "            \n",
    "            event_pred = self.event_model.predict(cnn_features)\n",
    "            agent_input = np.concatenate([cnn_features, event_pred], axis=1)\n",
    "            agent_pred = self.agent_model.predict(agent_input)\n",
    "            context_input = np.concatenate([cnn_features, event_pred, agent_pred], axis=1)\n",
    "            context_pred = self.context_model.predict(context_input)\n",
    "            \n",
    "            pred = {\n",
    "                'event_type': self.vocabularies['event_type'][np.argmax(event_pred[0])],\n",
    "                'agent_id': self.vocabularies['agent_id'][np.argmax(agent_pred[0])],\n",
    "                'context': self.vocabularies['context'][np.argmax(context_pred[0])]\n",
    "            }\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Update the input for the next prediction\n",
    "            current_input = self.update_input(current_input, pred)\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def update_input(self, current_input, prediction):\n",
    "        # This method should update the input grid based on the new prediction\n",
    "        # You'll need to implement the logic to update the grid based on your specific requirements\n",
    "        # For now, we'll just return the current input as a placeholder\n",
    "        return current_input\n",
    "\n",
    "# Main pipeline\n",
    "def process_event_log(file_path):\n",
    "    # Load and preprocess data\n",
    "    df = load_data(file_path)\n",
    "    processed_df, grid, vocabularies = preprocess_data(df)\n",
    "    \n",
    "    # Prepare target variables\n",
    "    y = {\n",
    "        'event_type': pd.get_dummies(processed_df['event_type']).values,\n",
    "        'agent_id': pd.get_dummies(processed_df['agent_id']).values,\n",
    "        'context': pd.get_dummies(processed_df['context']).values\n",
    "    }\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(grid, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and train hierarchical model\n",
    "    model = HierarchicalModel((3, 3, 1), vocabularies)\n",
    "    model.fit(X_train.reshape(-1, 3, 3, 1), y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    initial_input = X_test[0:1].reshape(-1, 3, 3, 1)\n",
    "    predictions = model.predict_sequence(initial_input, sequence_length=5)\n",
    "    \n",
    "    return predictions, X_test, y_test, vocabularies\n",
    "\n",
    "# Run the pipeline\n",
    "file_path = '10k_single_agent.csv'\n",
    "predictions, X_test, y_test, vocabularies = process_event_log(file_path)\n",
    "\n",
    "# Print the results\n",
    "print(\"Predicted sequence:\")\n",
    "for i, pred in enumerate(predictions):\n",
    "    print(f\"Step {i+1}: {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
