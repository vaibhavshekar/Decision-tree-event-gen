{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m20/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - agent_accuracy: 0.4200 - anomaly_accuracy: 0.4391 - context_accuracy: 0.0900 - loss: 2.8277 - next_event_accuracy: 0.6659Optimizer's learning rate is not a TensorFlow variable. Update the learning rate handling accordingly.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 142ms/step - agent_accuracy: 0.4232 - anomaly_accuracy: 0.4433 - context_accuracy: 0.0892 - loss: 2.8095 - next_event_accuracy: 0.6738 - val_agent_accuracy: 0.7113 - val_anomaly_accuracy: 0.5141 - val_context_accuracy: 0.0986 - val_loss: 2.3907 - val_next_event_accuracy: 0.8028\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 191\u001b[0m\n\u001b[0;32m    168\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m    169\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(), \n\u001b[0;32m    170\u001b[0m     loss\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    187\u001b[0m     }\n\u001b[0;32m    188\u001b[0m )\n\u001b[0;32m    190\u001b[0m \u001b[38;5;66;03m# Training the model\u001b[39;00m\n\u001b[1;32m--> 191\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m     \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m     \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m     \u001b[49m\u001b[43mtrain_sequences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnext_event\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manomaly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m         \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m         \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m         \u001b[49m\u001b[43mval_sequences\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnext_event\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43magent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43manomaly\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_targets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mCustomLossWeights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMetaLearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[0;32m    209\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[0;32m    210\u001b[0m     [test_sequences[:,:,\u001b[38;5;241m0\u001b[39m], test_sequences[:,:,\u001b[38;5;241m1\u001b[39m], test_sequences[:,:,\u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m    211\u001b[0m      test_sequences[:,:,\u001b[38;5;241m0\u001b[39m], test_sequences[:,:,\u001b[38;5;241m1\u001b[39m], test_sequences[:,:,\u001b[38;5;241m2\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    214\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_event\u001b[39m\u001b[38;5;124m'\u001b[39m: test_targets[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124magent\u001b[39m\u001b[38;5;124m'\u001b[39m: test_targets[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m: test_targets[\u001b[38;5;241m2\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124manomaly\u001b[39m\u001b[38;5;124m'\u001b[39m: test_targets[\u001b[38;5;241m3\u001b[39m]}\n\u001b[0;32m    215\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\vaibh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:329\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    328\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 329\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    330\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(\n\u001b[0;32m    331\u001b[0m         step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    332\u001b[0m     )\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not callable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Conv1D, GlobalMaxPooling1D, MultiHeadAttention, LayerNormalization, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import tensorflow as tf\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    event_type_mapping = df['event_type'].astype('category').cat.categories\n",
    "    agent_id_mapping = df['agent_id'].astype('category').cat.categories\n",
    "    context_mapping = df['context'].astype('category').cat.categories\n",
    "\n",
    "    df['event_type'] = df['event_type'].astype('category').cat.codes\n",
    "    df['agent_id'] = df['agent_id'].astype('category').cat.codes\n",
    "    df['context'] = df['context'].astype('category').cat.codes\n",
    "\n",
    "    return df, event_type_mapping, agent_id_mapping, context_mapping\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('1k_single_agent_minmax.csv')\n",
    "df, event_type_mapping, agent_id_mapping, context_mapping = preprocess_data(df)\n",
    "\n",
    "# Define constants\n",
    "num_classes = df['event_type'].nunique()\n",
    "num_agents = df['agent_id'].nunique()\n",
    "num_contexts = df['context'].nunique()\n",
    "sequence_length = 10  # Adjust based on your data\n",
    "num_features = df.shape[1]  # Number of features\n",
    "\n",
    "# Create sequences for training\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df.iloc[i:i+sequence_length].values\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "sequences = create_sequences(df, sequence_length)\n",
    "\n",
    "# Splitting the data\n",
    "train_size = int(0.7 * len(sequences))\n",
    "val_size = int(0.15 * len(sequences))\n",
    "train_sequences = sequences[:train_size]\n",
    "val_sequences = sequences[train_size:train_size+val_size]\n",
    "test_sequences = sequences[train_size+val_size:]\n",
    "\n",
    "# Extract targets\n",
    "def extract_targets(sequences):\n",
    "    next_event = to_categorical(sequences[:, -1, 0], num_classes=num_classes)\n",
    "    agent = to_categorical(sequences[:, -1, 1], num_classes=num_agents)\n",
    "    context = to_categorical(sequences[:, -1, 2], num_classes=num_contexts)\n",
    "    anomaly = np.random.randint(0, 2, size=(sequences.shape[0], 1))  # Placeholder for anomaly\n",
    "    return next_event, agent, context, anomaly\n",
    "\n",
    "train_targets = extract_targets(train_sequences)\n",
    "val_targets = extract_targets(val_sequences)\n",
    "test_targets = extract_targets(test_sequences)\n",
    "\n",
    "# Define the model branches\n",
    "def lstm_branch(input_shape):\n",
    "    inputs = Input(shape=(input_shape[0],))\n",
    "    x = Embedding(input_dim=num_classes, output_dim=64)(inputs)\n",
    "    x = LSTM(128, return_sequences=False)(x)\n",
    "    return inputs, x\n",
    "\n",
    "def transformer_branch(input_shape):\n",
    "    inputs = Input(shape=(input_shape[0],))\n",
    "    x = Embedding(input_dim=num_classes, output_dim=64)(inputs)\n",
    "    attn_output = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    x = LayerNormalization()(x + attn_output)\n",
    "    x = GlobalMaxPooling1D()(x)  # Apply GlobalMaxPooling1D to convert 3D to 2D\n",
    "    return inputs, x\n",
    "\n",
    "def convnet_branch(input_shape):\n",
    "    inputs = Input(shape=(input_shape[0],))\n",
    "    x = Embedding(input_dim=num_classes, output_dim=64)(inputs)\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    return inputs, x\n",
    "\n",
    "# Hierarchical Attention\n",
    "def hierarchical_attention_branch(input_shape):\n",
    "    inputs = Input(shape=(input_shape[0], input_shape[1]))\n",
    "    x = Attention()([inputs, inputs])\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    return inputs, x\n",
    "\n",
    "# Build the model\n",
    "def build_model(sequence_length):\n",
    "    # Define the branches for each input\n",
    "    lstm_event_inputs, lstm_event_output = lstm_branch((sequence_length,))\n",
    "    lstm_agent_inputs, lstm_agent_output = lstm_branch((sequence_length,))\n",
    "    lstm_context_inputs, lstm_context_output = lstm_branch((sequence_length,))\n",
    "    \n",
    "    transformer_event_inputs, transformer_event_output = transformer_branch((sequence_length,))\n",
    "    transformer_agent_inputs, transformer_agent_output = transformer_branch((sequence_length,))\n",
    "    transformer_context_inputs, transformer_context_output = transformer_branch((sequence_length,))\n",
    "    \n",
    "    convnet_event_inputs, convnet_event_output = convnet_branch((sequence_length,))\n",
    "    convnet_agent_inputs, convnet_agent_output = convnet_branch((sequence_length,))\n",
    "    convnet_context_inputs, convnet_context_output = convnet_branch((sequence_length,))\n",
    "    \n",
    "    # Hierarchical Attention\n",
    "    hier_attn_inputs, hier_attn_output = hierarchical_attention_branch((sequence_length, num_features))\n",
    "    \n",
    "    # Concatenate outputs from all branches\n",
    "    concatenated = Concatenate()([\n",
    "        lstm_event_output, lstm_agent_output, lstm_context_output,\n",
    "        transformer_event_output, transformer_agent_output, transformer_context_output,\n",
    "        convnet_event_output, convnet_agent_output, convnet_context_output,\n",
    "        hier_attn_output\n",
    "    ])\n",
    "    \n",
    "    shared_dense = Dense(128, activation='relu')(concatenated)\n",
    "    \n",
    "    next_event_head = Dense(num_classes, activation='softmax', name='next_event')(shared_dense)\n",
    "    agent_head = Dense(num_agents, activation='softmax', name='agent')(shared_dense)\n",
    "    context_head = Dense(num_contexts, activation='softmax', name='context')(shared_dense)\n",
    "    anomaly_head = Dense(1, activation='sigmoid', name='anomaly')(shared_dense)\n",
    "    \n",
    "    model = Model(inputs=[\n",
    "        lstm_event_inputs, lstm_agent_inputs, lstm_context_inputs,\n",
    "        transformer_event_inputs, transformer_agent_inputs, transformer_context_inputs,\n",
    "        convnet_event_inputs, convnet_agent_inputs, convnet_context_inputs,\n",
    "        hier_attn_inputs\n",
    "    ], outputs=[next_event_head, agent_head, context_head, anomaly_head])\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (sequence_length,)\n",
    "model = build_model(sequence_length)\n",
    "\n",
    "# Dynamic loss weighting using uncertainty weighting\n",
    "class CustomLossWeights(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        loss_weights = {\n",
    "            'next_event': 1.0,\n",
    "            'agent': 0.5,\n",
    "            'context': 0.5,\n",
    "            'anomaly': 0.1\n",
    "        }\n",
    "        # Update loss weights dynamically based on logs if necessary\n",
    "        for key in loss_weights.keys():\n",
    "            if f'{key}_loss' in logs:\n",
    "                loss_weights[key] = 1.0 / (logs[f'{key}_loss'] + 1e-5)\n",
    "        \n",
    "        self.model.compile(optimizer=self.model.optimizer, loss=self.model.loss, loss_weights=loss_weights)\n",
    "\n",
    "# Meta-learning inspired custom training loop\n",
    "class MetaLearning(Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs is None:\n",
    "            logs = {}\n",
    "        optimizer = self.model.optimizer\n",
    "        # Ensure learning rate is a TensorFlow variable\n",
    "        if isinstance(optimizer.learning_rate, tf.Variable):\n",
    "            lr = tf.keras.backend.get_value(optimizer.learning_rate)\n",
    "            new_lr = lr * (1 / (1 + 0.1 * epoch))\n",
    "            tf.keras.backend.set_value(optimizer.learning_rate, new_lr)\n",
    "        else:\n",
    "            print(\"Optimizer's learning rate is not a TensorFlow variable. Update the learning rate handling accordingly.\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), \n",
    "    loss={\n",
    "        'next_event': 'categorical_crossentropy', \n",
    "        'agent': 'categorical_crossentropy',\n",
    "        'context': 'categorical_crossentropy', \n",
    "        'anomaly': 'binary_crossentropy'\n",
    "    },\n",
    "    loss_weights={\n",
    "        'next_event': 1.0, \n",
    "        'agent': 0.5, \n",
    "        'context': 0.5, \n",
    "        'anomaly': 0.1\n",
    "    },\n",
    "    metrics={\n",
    "        'next_event': 'accuracy', \n",
    "        'agent': 'accuracy',\n",
    "        'context': 'accuracy',\n",
    "        'anomaly': 'accuracy'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    [train_sequences[:,:,0], train_sequences[:,:,1], train_sequences[:,:,2],\n",
    "     train_sequences[:,:,0], train_sequences[:,:,1], train_sequences[:,:,2],\n",
    "     train_sequences[:,:,0], train_sequences[:,:,1], train_sequences[:,:,2],\n",
    "     train_sequences],\n",
    "    {'next_event': train_targets[0], 'agent': train_targets[1], 'context': train_targets[2], 'anomaly': train_targets[3]},\n",
    "    validation_data=(\n",
    "        [val_sequences[:,:,0], val_sequences[:,:,1], val_sequences[:,:,2],\n",
    "         val_sequences[:,:,0], val_sequences[:,:,1], val_sequences[:,:,2],\n",
    "         val_sequences[:,:,0], val_sequences[:,:,1], val_sequences[:,:,2],\n",
    "         val_sequences],\n",
    "        {'next_event': val_targets[0], 'agent': val_targets[1], 'context': val_targets[2], 'anomaly': val_targets[3]}\n",
    "    ),\n",
    "    epochs=10, batch_size=32,\n",
    "    callbacks=[CustomLossWeights(), MetaLearning()]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = model.evaluate(\n",
    "    [test_sequences[:,:,0], test_sequences[:,:,1], test_sequences[:,:,2],\n",
    "     test_sequences[:,:,0], test_sequences[:,:,1], test_sequences[:,:,2],\n",
    "     test_sequences[:,:,0], test_sequences[:,:,1], test_sequences[:,:,2],\n",
    "     test_sequences],\n",
    "    {'next_event': test_targets[0], 'agent': test_targets[1], 'context': test_targets[2], 'anomaly': test_targets[3]}\n",
    ")\n",
    "print(f\"Test Loss and Accuracy: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 848ms/step\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,0']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,1']\n",
      " ['MOVE' 'O' '2,2']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']]\n",
      "Predicted next event_id: GAME_END\n",
      "Predicted next agent_id: system\n",
      "Predicted next context: O\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'X' '1,0']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,1']\n",
      " ['MOVE' 'O' '2,2']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']]\n",
      "Predicted next event_id: GAME_START\n",
      "Predicted next agent_id: system\n",
      "Predicted next context: New Game\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,1']\n",
      " ['MOVE' 'O' '2,2']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,1']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 1,2\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'X' '2,1']\n",
      " ['MOVE' 'O' '2,2']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,1']\n",
      " ['MOVE' 'O' '1,0']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: O\n",
      "Predicted next context: 2,2\n",
      "Predicted anomaly: [0]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'O' '2,2']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,1']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '0,1']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 0,1\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,1']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '2,1']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: O\n",
      "Predicted next context: 1,1\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,1']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '0,0']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 0,0\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,1']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: O\n",
      "Predicted next context: 0,2\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'O' '2,0']\n",
      " ['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,1']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['MOVE' 'X' '0,2']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 2,0\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['GAME_END' 'system' 'O']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '1,1']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '2,0']\n",
      " ['MOVE' 'X' '0,2']\n",
      " ['GAME_END' 'system' 'X']]\n",
      "Predicted next event_id: GAME_END\n",
      "Predicted next agent_id: system\n",
      "Predicted next context: X\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "sample_sequences = test_sequences[:num_samples]\n",
    "\n",
    "# Predict the next values\n",
    "predictions = model.predict(\n",
    "    [sample_sequences[:,:,0], sample_sequences[:,:,1], sample_sequences[:,:,2],\n",
    "     sample_sequences[:,:,0], sample_sequences[:,:,1], sample_sequences[:,:,2],\n",
    "     sample_sequences[:,:,0], sample_sequences[:,:,1], sample_sequences[:,:,2]]\n",
    ")\n",
    "\n",
    "# Extract predictions\n",
    "next_event_predictions = np.argmax(predictions[0], axis=-1)\n",
    "agent_predictions = np.argmax(predictions[1], axis=-1)\n",
    "context_predictions = np.argmax(predictions[2], axis=-1)\n",
    "anomaly_predictions = (predictions[3] > 0.5).astype(int)\n",
    "\n",
    "# Decode predictions\n",
    "def decode_predictions(predictions, id_to_label):\n",
    "    return [id_to_label[idx] for idx in predictions]\n",
    "\n",
    "decoded_next_event_predictions = decode_predictions(next_event_predictions, event_type_mapping)\n",
    "decoded_agent_predictions = decode_predictions(agent_predictions, agent_id_mapping)\n",
    "decoded_context_predictions = decode_predictions(context_predictions, context_mapping)\n",
    "\n",
    "# Decode input sequences\n",
    "def decode_sequences(sequences, event_mapping, agent_mapping, context_mapping):\n",
    "    decoded_sequences = []\n",
    "    for seq in sequences:\n",
    "        decoded_seq = []\n",
    "        for step in seq:\n",
    "            decoded_step = [\n",
    "                event_mapping[step[0]],\n",
    "                agent_mapping[step[1]],\n",
    "                context_mapping[step[2]]\n",
    "            ]\n",
    "            decoded_seq.append(decoded_step)\n",
    "        decoded_sequences.append(decoded_seq)\n",
    "    return np.array(decoded_sequences)\n",
    "\n",
    "decoded_sample_sequences = decode_sequences(sample_sequences, event_type_mapping, agent_id_mapping, context_mapping)\n",
    "\n",
    "# Print decoded input sequences and predictions\n",
    "for i in range(num_samples):\n",
    "    print(f\"Input sequence (event_id, agent_id, context):\\n{decoded_sample_sequences[i]}\")\n",
    "    print(f\"Predicted next event_id: {decoded_next_event_predictions[i]}\")\n",
    "    print(f\"Predicted next agent_id: {decoded_agent_predictions[i]}\")\n",
    "    print(f\"Predicted next context: {decoded_context_predictions[i]}\")\n",
    "    print(f\"Predicted anomaly: {anomaly_predictions[i]}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
