{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 385ms/step - agent_accuracy: 0.2828 - anomaly_accuracy: 0.5312 - context_accuracy: 0.0709 - loss: 3.3923 - next_event_accuracy: 0.3425 - val_agent_accuracy: 0.4667 - val_anomaly_accuracy: 0.4000 - val_context_accuracy: 0.0667 - val_loss: 2.2884 - val_next_event_accuracy: 0.8667\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - agent_accuracy: 0.4590 - anomaly_accuracy: 0.4779 - context_accuracy: 0.1151 - loss: 2.3784 - next_event_accuracy: 0.8219 - val_agent_accuracy: 0.4667 - val_anomaly_accuracy: 0.7333 - val_context_accuracy: 0.0667 - val_loss: 2.2020 - val_next_event_accuracy: 0.8667\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - agent_accuracy: 0.4200 - anomaly_accuracy: 0.4648 - context_accuracy: 0.1372 - loss: 2.3400 - next_event_accuracy: 0.7828 - val_agent_accuracy: 0.4667 - val_anomaly_accuracy: 0.6000 - val_context_accuracy: 0.2000 - val_loss: 2.0997 - val_next_event_accuracy: 0.8667\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - agent_accuracy: 0.4708 - anomaly_accuracy: 0.4779 - context_accuracy: 0.2022 - loss: 2.2023 - next_event_accuracy: 0.7906 - val_agent_accuracy: 0.4667 - val_anomaly_accuracy: 0.2667 - val_context_accuracy: 0.2000 - val_loss: 2.0277 - val_next_event_accuracy: 0.8667\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - agent_accuracy: 0.4590 - anomaly_accuracy: 0.4733 - context_accuracy: 0.2048 - loss: 2.1929 - next_event_accuracy: 0.7828 - val_agent_accuracy: 0.8667 - val_anomaly_accuracy: 0.2667 - val_context_accuracy: 0.2667 - val_loss: 1.9224 - val_next_event_accuracy: 0.8667\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - agent_accuracy: 0.6652 - anomaly_accuracy: 0.5110 - context_accuracy: 0.2471 - loss: 2.0827 - next_event_accuracy: 0.7750 - val_agent_accuracy: 0.6000 - val_anomaly_accuracy: 0.2667 - val_context_accuracy: 0.2667 - val_loss: 1.8297 - val_next_event_accuracy: 0.8667\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - agent_accuracy: 0.5410 - anomaly_accuracy: 0.5234 - context_accuracy: 0.2471 - loss: 1.8228 - next_event_accuracy: 0.8212 - val_agent_accuracy: 0.6667 - val_anomaly_accuracy: 0.2667 - val_context_accuracy: 0.2000 - val_loss: 1.7241 - val_next_event_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - agent_accuracy: 0.6515 - anomaly_accuracy: 0.5455 - context_accuracy: 0.3166 - loss: 1.6897 - next_event_accuracy: 1.0000 - val_agent_accuracy: 0.6667 - val_anomaly_accuracy: 0.3333 - val_context_accuracy: 0.4000 - val_loss: 1.5741 - val_next_event_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - agent_accuracy: 0.7548 - anomaly_accuracy: 0.5189 - context_accuracy: 0.4701 - loss: 1.5122 - next_event_accuracy: 1.0000 - val_agent_accuracy: 0.7333 - val_anomaly_accuracy: 0.4000 - val_context_accuracy: 0.3333 - val_loss: 1.4289 - val_next_event_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - agent_accuracy: 0.7191 - anomaly_accuracy: 0.5000 - context_accuracy: 0.3725 - loss: 1.3892 - next_event_accuracy: 1.0000 - val_agent_accuracy: 0.7333 - val_anomaly_accuracy: 0.4000 - val_context_accuracy: 0.3333 - val_loss: 1.3136 - val_next_event_accuracy: 1.0000\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - agent_accuracy: 0.6667 - anomaly_accuracy: 0.6000 - context_accuracy: 0.3333 - loss: 1.3447 - next_event_accuracy: 1.0000\n",
      "Test Loss and Accuracy: [1.3447458744049072, 0.6666666865348816, 0.6000000238418579, 0.3333333432674408, 1.0]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Conv1D, GlobalMaxPooling1D, MultiHeadAttention, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Data Preprocessing\n",
    "def preprocess_data(df):\n",
    "    event_type_mapping = df['event_type'].astype('category').cat.categories\n",
    "    agent_id_mapping = df['agent_id'].astype('category').cat.categories\n",
    "    context_mapping = df['context'].astype('category').cat.categories\n",
    "\n",
    "    df['event_type'] = df['event_type'].astype('category').cat.codes\n",
    "    df['agent_id'] = df['agent_id'].astype('category').cat.codes\n",
    "    df['context'] = df['context'].astype('category').cat.codes\n",
    "\n",
    "    return df, event_type_mapping, agent_id_mapping, context_mapping\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('tictactoe_logs.csv')\n",
    "df, event_type_mapping, agent_id_mapping, context_mapping = preprocess_data(df)\n",
    "\n",
    "# Define constants\n",
    "num_classes = df['event_type'].nunique()\n",
    "num_agents = df['agent_id'].nunique()\n",
    "num_contexts = df['context'].nunique()\n",
    "sequence_length = 10  # Adjust based on your data\n",
    "num_features = df.shape[1]  # Number of features\n",
    "\n",
    "# Create sequences for training\n",
    "def create_sequences(df, sequence_length):\n",
    "    sequences = []\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        seq = df.iloc[i:i+sequence_length].values\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "sequences = create_sequences(df, sequence_length)\n",
    "\n",
    "# Splitting the data\n",
    "train_size = int(0.7 * len(sequences))\n",
    "val_size = int(0.15 * len(sequences))\n",
    "train_sequences = sequences[:train_size]\n",
    "val_sequences = sequences[train_size:train_size+val_size]\n",
    "test_sequences = sequences[train_size+val_size:]\n",
    "\n",
    "# Extract targets\n",
    "def extract_targets(sequences):\n",
    "    next_event = to_categorical(sequences[:, -1, 0], num_classes=num_classes)\n",
    "    agent = to_categorical(sequences[:, -1, 1], num_classes=num_agents)\n",
    "    context = to_categorical(sequences[:, -1, 2], num_classes=num_contexts)\n",
    "    anomaly = np.random.randint(0, 2, size=(sequences.shape[0], 1))  # Placeholder for anomaly\n",
    "    return next_event, agent, context, anomaly\n",
    "\n",
    "train_targets = extract_targets(train_sequences)\n",
    "val_targets = extract_targets(val_sequences)\n",
    "test_targets = extract_targets(test_sequences)\n",
    "\n",
    "# Define the model branches\n",
    "def lstm_branch(input_shape):\n",
    "    inputs = Input(shape=(input_shape[0],))\n",
    "    x = Embedding(input_dim=num_classes, output_dim=64)(inputs)\n",
    "    x = LSTM(128, return_sequences=False)(x)\n",
    "    return inputs, x\n",
    "\n",
    "def transformer_branch(input_shape):\n",
    "    inputs = Input(shape=(input_shape[0],))\n",
    "    x = Embedding(input_dim=num_classes, output_dim=64)(inputs)\n",
    "    attn_output = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    x = LayerNormalization()(x + attn_output)\n",
    "    x = GlobalMaxPooling1D()(x)  # Apply GlobalMaxPooling1D to convert 3D to 2D\n",
    "    return inputs, x\n",
    "\n",
    "def convnet_branch(input_shape):\n",
    "    inputs = Input(shape=(input_shape[0],))\n",
    "    x = Embedding(input_dim=num_classes, output_dim=64)(inputs)\n",
    "    x = Conv1D(128, kernel_size=3, activation='relu')(x)\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "    return inputs, x\n",
    "\n",
    "# Build the model\n",
    "def build_model(sequence_length):\n",
    "    # Define the branches for each input\n",
    "    lstm_event_inputs, lstm_event_output = lstm_branch((sequence_length,))\n",
    "    lstm_agent_inputs, lstm_agent_output = lstm_branch((sequence_length,))\n",
    "    lstm_context_inputs, lstm_context_output = lstm_branch((sequence_length,))\n",
    "    \n",
    "    transformer_event_inputs, transformer_event_output = transformer_branch((sequence_length,))\n",
    "    transformer_agent_inputs, transformer_agent_output = transformer_branch((sequence_length,))\n",
    "    transformer_context_inputs, transformer_context_output = transformer_branch((sequence_length,))\n",
    "    \n",
    "    convnet_event_inputs, convnet_event_output = convnet_branch((sequence_length,))\n",
    "    convnet_agent_inputs, convnet_agent_output = convnet_branch((sequence_length,))\n",
    "    convnet_context_inputs, convnet_context_output = convnet_branch((sequence_length,))\n",
    "    \n",
    "    # Concatenate outputs from all branches\n",
    "    concatenated = Concatenate()([\n",
    "        lstm_event_output, lstm_agent_output, lstm_context_output,\n",
    "        transformer_event_output, transformer_agent_output, transformer_context_output,\n",
    "        convnet_event_output, convnet_agent_output, convnet_context_output\n",
    "    ])\n",
    "    \n",
    "    shared_dense = Dense(128, activation='relu')(concatenated)\n",
    "    \n",
    "    next_event_head = Dense(num_classes, activation='softmax', name='next_event')(shared_dense)\n",
    "    agent_head = Dense(num_agents, activation='softmax', name='agent')(shared_dense)\n",
    "    context_head = Dense(num_contexts, activation='softmax', name='context')(shared_dense)\n",
    "    anomaly_head = Dense(1, activation='sigmoid', name='anomaly')(shared_dense)\n",
    "    \n",
    "    model = Model(inputs=[\n",
    "        lstm_event_inputs, lstm_agent_inputs, lstm_context_inputs,\n",
    "        transformer_event_inputs, transformer_agent_inputs, transformer_context_inputs,\n",
    "        convnet_event_inputs, convnet_agent_inputs, convnet_context_inputs\n",
    "    ], outputs=[next_event_head, agent_head, context_head, anomaly_head])\n",
    "    \n",
    "    return model\n",
    "\n",
    "input_shape = (sequence_length,)\n",
    "model = build_model(sequence_length)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'next_event': 'categorical_crossentropy', \n",
    "                    'agent': 'categorical_crossentropy',\n",
    "                    'context': 'categorical_crossentropy', \n",
    "                    'anomaly': 'binary_crossentropy'},\n",
    "              loss_weights={'next_event': 1.0, 'agent': 0.5, 'context': 0.5, 'anomaly': 0.1},\n",
    "              metrics={'next_event': 'accuracy', \n",
    "                       'agent': 'accuracy',\n",
    "                       'context': 'accuracy',\n",
    "                       'anomaly': 'accuracy'})\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    [train_sequences[:,:,0], train_sequences[:,:,1], train_sequences[:,:,2],\n",
    "     train_sequences[:,:,0], train_sequences[:,:,1], train_sequences[:,:,2],\n",
    "     train_sequences[:,:,0], train_sequences[:,:,1], train_sequences[:,:,2]],\n",
    "    {'next_event': train_targets[0], 'agent': train_targets[1], 'context': train_targets[2], 'anomaly': train_targets[3]},\n",
    "    validation_data=(\n",
    "        [val_sequences[:,:,0], val_sequences[:,:,1], val_sequences[:,:,2],\n",
    "         val_sequences[:,:,0], val_sequences[:,:,1], val_sequences[:,:,2],\n",
    "         val_sequences[:,:,0], val_sequences[:,:,1], val_sequences[:,:,2]],\n",
    "        {'next_event': val_targets[0], 'agent': val_targets[1], 'context': val_targets[2], 'anomaly': val_targets[3]}\n",
    "    ),\n",
    "    epochs=10, batch_size=32\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = model.evaluate(\n",
    "    [test_sequences[:,:,0], test_sequences[:,:,1], test_sequences[:,:,2],\n",
    "     test_sequences[:,:,0], test_sequences[:,:,1], test_sequences[:,:,2],\n",
    "     test_sequences[:,:,0], test_sequences[:,:,1], test_sequences[:,:,2]],\n",
    "    {'next_event': test_targets[0], 'agent': test_targets[1], 'context': test_targets[2], 'anomaly': test_targets[3]}\n",
    ")\n",
    "print(f\"Test Loss and Accuracy: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '2,2']\n",
      " ['GAME_END' 'system' 'draw']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 2,0\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'X' '2,2']\n",
      " ['GAME_END' 'system' 'draw']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 2,0\n",
      "Predicted anomaly: [0]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['GAME_END' 'system' 'draw']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '2,1']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 2,0\n",
      "Predicted anomaly: [0]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '2,2']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 2,2\n",
      "Predicted anomaly: [0]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '2,2']\n",
      " ['GAME_END' 'system' 'draw']]\n",
      "Predicted next event_id: GAME_END\n",
      "Predicted next agent_id: system\n",
      "Predicted next context: draw\n",
      "Predicted anomaly: [0]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '2,2']\n",
      " ['GAME_END' 'system' 'draw']\n",
      " ['GAME_START' 'system' 'New Game']]\n",
      "Predicted next event_id: GAME_START\n",
      "Predicted next agent_id: system\n",
      "Predicted next context: New Game\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '2,2']\n",
      " ['GAME_END' 'system' 'draw']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '0,0']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 2,0\n",
      "Predicted anomaly: [0]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'O' '0,2']\n",
      " ['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '2,2']\n",
      " ['GAME_END' 'system' 'draw']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '1,1']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: O\n",
      "Predicted next context: 2,0\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'X' '2,0']\n",
      " ['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '2,2']\n",
      " ['GAME_END' 'system' 'draw']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,1']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 2,0\n",
      "Predicted anomaly: [0]\n",
      "------------------------------\n",
      "Input sequence (event_id, agent_id, context):\n",
      "[['MOVE' 'O' '1,0']\n",
      " ['MOVE' 'X' '1,2']\n",
      " ['MOVE' 'O' '2,1']\n",
      " ['MOVE' 'X' '2,2']\n",
      " ['GAME_END' 'system' 'draw']\n",
      " ['GAME_START' 'system' 'New Game']\n",
      " ['MOVE' 'X' '0,0']\n",
      " ['MOVE' 'O' '1,1']\n",
      " ['MOVE' 'X' '0,1']\n",
      " ['MOVE' 'O' '0,2']]\n",
      "Predicted next event_id: MOVE\n",
      "Predicted next agent_id: X\n",
      "Predicted next context: 2,0\n",
      "Predicted anomaly: [1]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_samples = 10\n",
    "sample_sequences = test_sequences[:num_samples]\n",
    "\n",
    "# Predict the next values\n",
    "predictions = model.predict(\n",
    "    [sample_sequences[:,:,0], sample_sequences[:,:,1], sample_sequences[:,:,2],\n",
    "     sample_sequences[:,:,0], sample_sequences[:,:,1], sample_sequences[:,:,2],\n",
    "     sample_sequences[:,:,0], sample_sequences[:,:,1], sample_sequences[:,:,2]]\n",
    ")\n",
    "\n",
    "# Extract predictions\n",
    "next_event_predictions = np.argmax(predictions[0], axis=-1)\n",
    "agent_predictions = np.argmax(predictions[1], axis=-1)\n",
    "context_predictions = np.argmax(predictions[2], axis=-1)\n",
    "anomaly_predictions = (predictions[3] > 0.5).astype(int)\n",
    "\n",
    "# Decode predictions\n",
    "def decode_predictions(predictions, id_to_label):\n",
    "    return [id_to_label[idx] for idx in predictions]\n",
    "\n",
    "decoded_next_event_predictions = decode_predictions(next_event_predictions, event_type_mapping)\n",
    "decoded_agent_predictions = decode_predictions(agent_predictions, agent_id_mapping)\n",
    "decoded_context_predictions = decode_predictions(context_predictions, context_mapping)\n",
    "\n",
    "# Decode input sequences\n",
    "def decode_sequences(sequences, event_mapping, agent_mapping, context_mapping):\n",
    "    decoded_sequences = []\n",
    "    for seq in sequences:\n",
    "        decoded_seq = []\n",
    "        for step in seq:\n",
    "            decoded_step = [\n",
    "                event_mapping[step[0]],\n",
    "                agent_mapping[step[1]],\n",
    "                context_mapping[step[2]]\n",
    "            ]\n",
    "            decoded_seq.append(decoded_step)\n",
    "        decoded_sequences.append(decoded_seq)\n",
    "    return np.array(decoded_sequences)\n",
    "\n",
    "decoded_sample_sequences = decode_sequences(sample_sequences, event_type_mapping, agent_id_mapping, context_mapping)\n",
    "\n",
    "# Print decoded input sequences and predictions\n",
    "for i in range(num_samples):\n",
    "    print(f\"Input sequence (event_id, agent_id, context):\\n{decoded_sample_sequences[i]}\")\n",
    "    print(f\"Predicted next event_id: {decoded_next_event_predictions[i]}\")\n",
    "    print(f\"Predicted next agent_id: {decoded_agent_predictions[i]}\")\n",
    "    print(f\"Predicted next context: {decoded_context_predictions[i]}\")\n",
    "    print(f\"Predicted anomaly: {anomaly_predictions[i]}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
